{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\n",
      "Untitled1.ipynb\n",
      "hw1\n",
      "hw2\n",
      "iris.data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "print(check_output([\"ls\", \"./\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5.1</th>\n",
       "      <th>3.5</th>\n",
       "      <th>1.4</th>\n",
       "      <th>0.2</th>\n",
       "      <th>Iris-setosa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     5.1  3.5  1.4  0.2     Iris-setosa\n",
       "0    4.9  3.0  1.4  0.2     Iris-setosa\n",
       "1    4.7  3.2  1.3  0.2     Iris-setosa\n",
       "2    4.6  3.1  1.5  0.2     Iris-setosa\n",
       "3    5.0  3.6  1.4  0.2     Iris-setosa\n",
       "4    5.4  3.9  1.7  0.4     Iris-setosa\n",
       "5    4.6  3.4  1.4  0.3     Iris-setosa\n",
       "6    5.0  3.4  1.5  0.2     Iris-setosa\n",
       "7    4.4  2.9  1.4  0.2     Iris-setosa\n",
       "8    4.9  3.1  1.5  0.1     Iris-setosa\n",
       "9    5.4  3.7  1.5  0.2     Iris-setosa\n",
       "10   4.8  3.4  1.6  0.2     Iris-setosa\n",
       "11   4.8  3.0  1.4  0.1     Iris-setosa\n",
       "12   4.3  3.0  1.1  0.1     Iris-setosa\n",
       "13   5.8  4.0  1.2  0.2     Iris-setosa\n",
       "14   5.7  4.4  1.5  0.4     Iris-setosa\n",
       "15   5.4  3.9  1.3  0.4     Iris-setosa\n",
       "16   5.1  3.5  1.4  0.3     Iris-setosa\n",
       "17   5.7  3.8  1.7  0.3     Iris-setosa\n",
       "18   5.1  3.8  1.5  0.3     Iris-setosa\n",
       "19   5.4  3.4  1.7  0.2     Iris-setosa\n",
       "20   5.1  3.7  1.5  0.4     Iris-setosa\n",
       "21   4.6  3.6  1.0  0.2     Iris-setosa\n",
       "22   5.1  3.3  1.7  0.5     Iris-setosa\n",
       "23   4.8  3.4  1.9  0.2     Iris-setosa\n",
       "24   5.0  3.0  1.6  0.2     Iris-setosa\n",
       "25   5.0  3.4  1.6  0.4     Iris-setosa\n",
       "26   5.2  3.5  1.5  0.2     Iris-setosa\n",
       "27   5.2  3.4  1.4  0.2     Iris-setosa\n",
       "28   4.7  3.2  1.6  0.2     Iris-setosa\n",
       "29   4.8  3.1  1.6  0.2     Iris-setosa\n",
       "..   ...  ...  ...  ...             ...\n",
       "119  6.9  3.2  5.7  2.3  Iris-virginica\n",
       "120  5.6  2.8  4.9  2.0  Iris-virginica\n",
       "121  7.7  2.8  6.7  2.0  Iris-virginica\n",
       "122  6.3  2.7  4.9  1.8  Iris-virginica\n",
       "123  6.7  3.3  5.7  2.1  Iris-virginica\n",
       "124  7.2  3.2  6.0  1.8  Iris-virginica\n",
       "125  6.2  2.8  4.8  1.8  Iris-virginica\n",
       "126  6.1  3.0  4.9  1.8  Iris-virginica\n",
       "127  6.4  2.8  5.6  2.1  Iris-virginica\n",
       "128  7.2  3.0  5.8  1.6  Iris-virginica\n",
       "129  7.4  2.8  6.1  1.9  Iris-virginica\n",
       "130  7.9  3.8  6.4  2.0  Iris-virginica\n",
       "131  6.4  2.8  5.6  2.2  Iris-virginica\n",
       "132  6.3  2.8  5.1  1.5  Iris-virginica\n",
       "133  6.1  2.6  5.6  1.4  Iris-virginica\n",
       "134  7.7  3.0  6.1  2.3  Iris-virginica\n",
       "135  6.3  3.4  5.6  2.4  Iris-virginica\n",
       "136  6.4  3.1  5.5  1.8  Iris-virginica\n",
       "137  6.0  3.0  4.8  1.8  Iris-virginica\n",
       "138  6.9  3.1  5.4  2.1  Iris-virginica\n",
       "139  6.7  3.1  5.6  2.4  Iris-virginica\n",
       "140  6.9  3.1  5.1  2.3  Iris-virginica\n",
       "141  5.8  2.7  5.1  1.9  Iris-virginica\n",
       "142  6.8  3.2  5.9  2.3  Iris-virginica\n",
       "143  6.7  3.3  5.7  2.5  Iris-virginica\n",
       "144  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "145  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "146  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "147  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "148  5.9  3.0  5.1  1.8  Iris-virginica\n",
       "\n",
       "[149 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "dataset = pd.read_csv('./iris.data')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and test test\n",
    "X = dataset.iloc[:,1:4].values\n",
    "y = dataset.iloc[:,4].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder =  LabelEncoder()\n",
    "y1 = encoder.fit_transform(y)\n",
    "Y = pd.get_dummies(y1).values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=0) \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_scaler = MinMaxScaler()\n",
    "X_train = x_scaler.fit_transform(X_train)\n",
    "X_test = x_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining the model \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD,Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(10,input_shape=(4,),activation='tanh'))\n",
    "# model.add(Dense(8,activation='tanh'))\n",
    "# model.add(Dense(6,activation='tanh'))\n",
    "model.add(Dense(3,input_shape=(3,),activation='softmax'))\n",
    "\n",
    "# small weight\n",
    "# weight1 = np.zeros((3, 3))\n",
    "# weight2 = np.zeros(3)\n",
    "\n",
    "# larget weight\n",
    "weight1 = np.full((3, 3), 65536)\n",
    "weight2 = np.full((3), 65536)\n",
    "\n",
    "weight = [weight1, weight2]\n",
    "model.set_weights(weight)\n",
    "\n",
    "# model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119 samples, validate on 30 samples\n",
      "Epoch 1/150\n",
      "119/119 [==============================] - 0s 3ms/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 2/150\n",
      "119/119 [==============================] - 0s 100us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 3/150\n",
      "119/119 [==============================] - 0s 133us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 4/150\n",
      "119/119 [==============================] - 0s 131us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 5/150\n",
      "119/119 [==============================] - 0s 185us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 6/150\n",
      "119/119 [==============================] - 0s 163us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 7/150\n",
      "119/119 [==============================] - 0s 219us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 8/150\n",
      "119/119 [==============================] - 0s 188us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 9/150\n",
      "119/119 [==============================] - 0s 178us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 10/150\n",
      "119/119 [==============================] - 0s 169us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 11/150\n",
      "119/119 [==============================] - 0s 149us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 12/150\n",
      "119/119 [==============================] - 0s 100us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 13/150\n",
      "119/119 [==============================] - 0s 142us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0986 - val_acc: 0.4000\n",
      "Epoch 14/150\n",
      "119/119 [==============================] - 0s 107us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 15/150\n",
      "119/119 [==============================] - 0s 139us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 16/150\n",
      "119/119 [==============================] - 0s 118us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 17/150\n",
      "119/119 [==============================] - 0s 179us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 18/150\n",
      "119/119 [==============================] - 0s 143us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 19/150\n",
      "119/119 [==============================] - 0s 155us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 20/150\n",
      "119/119 [==============================] - 0s 121us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 21/150\n",
      "119/119 [==============================] - 0s 144us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 22/150\n",
      "119/119 [==============================] - 0s 139us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 23/150\n",
      "119/119 [==============================] - 0s 132us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 24/150\n",
      "119/119 [==============================] - 0s 138us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 25/150\n",
      "119/119 [==============================] - 0s 134us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 26/150\n",
      "119/119 [==============================] - 0s 128us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 27/150\n",
      "119/119 [==============================] - 0s 143us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 28/150\n",
      "119/119 [==============================] - 0s 138us/step - loss: 1.0986 - acc: 0.3109 - val_loss: 1.0987 - val_acc: 0.4000\n",
      "Epoch 29/150\n",
      "119/119 [==============================] - 0s 150us/step - loss: 1.0980 - acc: 0.3109 - val_loss: 1.0981 - val_acc: 0.4000\n",
      "Epoch 30/150\n",
      "119/119 [==============================] - 0s 127us/step - loss: 1.0974 - acc: 0.3950 - val_loss: 1.0981 - val_acc: 0.4000\n",
      "Epoch 31/150\n",
      "119/119 [==============================] - 0s 151us/step - loss: 1.0974 - acc: 0.3950 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 32/150\n",
      "119/119 [==============================] - 0s 116us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 33/150\n",
      "119/119 [==============================] - 0s 156us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 34/150\n",
      "119/119 [==============================] - 0s 154us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 35/150\n",
      "119/119 [==============================] - 0s 138us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 36/150\n",
      "119/119 [==============================] - 0s 119us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 37/150\n",
      "119/119 [==============================] - 0s 153us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 38/150\n",
      "119/119 [==============================] - 0s 116us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 39/150\n",
      "119/119 [==============================] - 0s 160us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 40/150\n",
      "119/119 [==============================] - 0s 166us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 41/150\n",
      "119/119 [==============================] - 0s 162us/step - loss: 1.0974 - acc: 0.3445 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 42/150\n",
      "119/119 [==============================] - 0s 138us/step - loss: 1.0975 - acc: 0.3445 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 43/150\n",
      "119/119 [==============================] - 0s 190us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 44/150\n",
      "119/119 [==============================] - 0s 127us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 45/150\n",
      "119/119 [==============================] - 0s 151us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 46/150\n",
      "119/119 [==============================] - 0s 105us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 47/150\n",
      "119/119 [==============================] - 0s 154us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 48/150\n",
      "119/119 [==============================] - 0s 113us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 49/150\n",
      "119/119 [==============================] - 0s 119us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 50/150\n",
      "119/119 [==============================] - 0s 145us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 51/150\n",
      "119/119 [==============================] - 0s 104us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 52/150\n",
      "119/119 [==============================] - 0s 144us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 53/150\n",
      "119/119 [==============================] - 0s 156us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0989 - val_acc: 0.3667\n",
      "Epoch 54/150\n",
      "119/119 [==============================] - 0s 103us/step - loss: 1.0974 - acc: 0.3866 - val_loss: 1.0990 - val_acc: 0.2667\n",
      "Epoch 55/150\n",
      "119/119 [==============================] - 0s 156us/step - loss: 1.0974 - acc: 0.3361 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 56/150\n",
      "119/119 [==============================] - 0s 122us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 57/150\n",
      "119/119 [==============================] - 0s 127us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 58/150\n",
      "119/119 [==============================] - 0s 107us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 59/150\n",
      "119/119 [==============================] - 0s 128us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 60/150\n",
      "119/119 [==============================] - 0s 115us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/150\n",
      "119/119 [==============================] - 0s 117us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 62/150\n",
      "119/119 [==============================] - 0s 115us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 63/150\n",
      "119/119 [==============================] - 0s 133us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 64/150\n",
      "119/119 [==============================] - 0s 145us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 65/150\n",
      "119/119 [==============================] - 0s 127us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 66/150\n",
      "119/119 [==============================] - 0s 126us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 67/150\n",
      "119/119 [==============================] - 0s 134us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 68/150\n",
      "119/119 [==============================] - 0s 134us/step - loss: 1.0971 - acc: 0.3445 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 69/150\n",
      "119/119 [==============================] - 0s 136us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 70/150\n",
      "119/119 [==============================] - 0s 129us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 71/150\n",
      "119/119 [==============================] - 0s 129us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 72/150\n",
      "119/119 [==============================] - 0s 141us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 73/150\n",
      "119/119 [==============================] - 0s 136us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 74/150\n",
      "119/119 [==============================] - 0s 141us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 75/150\n",
      "119/119 [==============================] - 0s 155us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 76/150\n",
      "119/119 [==============================] - 0s 154us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 77/150\n",
      "119/119 [==============================] - 0s 210us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 78/150\n",
      "119/119 [==============================] - 0s 205us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 79/150\n",
      "119/119 [==============================] - 0s 204us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 80/150\n",
      "119/119 [==============================] - 0s 160us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 81/150\n",
      "119/119 [==============================] - 0s 163us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 82/150\n",
      "119/119 [==============================] - 0s 162us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 83/150\n",
      "119/119 [==============================] - 0s 122us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 84/150\n",
      "119/119 [==============================] - 0s 124us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 85/150\n",
      "119/119 [==============================] - 0s 138us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 86/150\n",
      "119/119 [==============================] - 0s 132us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 87/150\n",
      "119/119 [==============================] - 0s 148us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 88/150\n",
      "119/119 [==============================] - 0s 145us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 89/150\n",
      "119/119 [==============================] - 0s 145us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 90/150\n",
      "119/119 [==============================] - 0s 153us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 91/150\n",
      "119/119 [==============================] - 0s 113us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 92/150\n",
      "119/119 [==============================] - 0s 139us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 93/150\n",
      "119/119 [==============================] - 0s 166us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 94/150\n",
      "119/119 [==============================] - 0s 139us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 95/150\n",
      "119/119 [==============================] - 0s 122us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0980 - val_acc: 0.3000\n",
      "Epoch 96/150\n",
      "119/119 [==============================] - 0s 148us/step - loss: 1.0970 - acc: 0.3866 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 97/150\n",
      "119/119 [==============================] - 0s 125us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 98/150\n",
      "119/119 [==============================] - 0s 130us/step - loss: 1.0968 - acc: 0.3277 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 99/150\n",
      "119/119 [==============================] - 0s 145us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 100/150\n",
      "119/119 [==============================] - 0s 148us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 101/150\n",
      "119/119 [==============================] - 0s 125us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 102/150\n",
      "119/119 [==============================] - 0s 133us/step - loss: 1.0967 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 103/150\n",
      "119/119 [==============================] - 0s 129us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 104/150\n",
      "119/119 [==============================] - 0s 143us/step - loss: 1.0968 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 105/150\n",
      "119/119 [==============================] - 0s 123us/step - loss: 1.0968 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 106/150\n",
      "119/119 [==============================] - 0s 119us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 107/150\n",
      "119/119 [==============================] - 0s 139us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 108/150\n",
      "119/119 [==============================] - 0s 132us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 109/150\n",
      "119/119 [==============================] - 0s 126us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 110/150\n",
      "119/119 [==============================] - 0s 124us/step - loss: 1.0968 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 111/150\n",
      "119/119 [==============================] - 0s 126us/step - loss: 1.0968 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 112/150\n",
      "119/119 [==============================] - 0s 126us/step - loss: 1.0968 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 113/150\n",
      "119/119 [==============================] - 0s 132us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 114/150\n",
      "119/119 [==============================] - 0s 120us/step - loss: 1.0968 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 115/150\n",
      "119/119 [==============================] - 0s 133us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 116/150\n",
      "119/119 [==============================] - 0s 118us/step - loss: 1.0968 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 117/150\n",
      "119/119 [==============================] - 0s 130us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 118/150\n",
      "119/119 [==============================] - 0s 107us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 119/150\n",
      "119/119 [==============================] - 0s 109us/step - loss: 1.0968 - acc: 0.3277 - val_loss: 1.0981 - val_acc: 0.3000\n",
      "Epoch 120/150\n",
      "119/119 [==============================] - 0s 112us/step - loss: 1.0969 - acc: 0.3193 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 0s 121us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 122/150\n",
      "119/119 [==============================] - 0s 111us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 123/150\n",
      "119/119 [==============================] - 0s 121us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 124/150\n",
      "119/119 [==============================] - 0s 122us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 125/150\n",
      "119/119 [==============================] - 0s 139us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 126/150\n",
      "119/119 [==============================] - 0s 136us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 127/150\n",
      "119/119 [==============================] - 0s 136us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 128/150\n",
      "119/119 [==============================] - 0s 142us/step - loss: 1.0954 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 129/150\n",
      "119/119 [==============================] - 0s 217us/step - loss: 1.0957 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 130/150\n",
      "119/119 [==============================] - 0s 163us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 131/150\n",
      "119/119 [==============================] - 0s 160us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 132/150\n",
      "119/119 [==============================] - 0s 133us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 133/150\n",
      "119/119 [==============================] - 0s 131us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 134/150\n",
      "119/119 [==============================] - 0s 155us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 135/150\n",
      "119/119 [==============================] - 0s 144us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 136/150\n",
      "119/119 [==============================] - 0s 131us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 137/150\n",
      "119/119 [==============================] - 0s 143us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 138/150\n",
      "119/119 [==============================] - 0s 134us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 139/150\n",
      "119/119 [==============================] - 0s 157us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 140/150\n",
      "119/119 [==============================] - 0s 128us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 141/150\n",
      "119/119 [==============================] - 0s 138us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0984 - val_acc: 0.3333\n",
      "Epoch 142/150\n",
      "119/119 [==============================] - 0s 134us/step - loss: 1.0956 - acc: 0.3361 - val_loss: 1.0985 - val_acc: 0.2333\n",
      "Epoch 143/150\n",
      "119/119 [==============================] - 0s 156us/step - loss: 1.0954 - acc: 0.2941 - val_loss: 1.0985 - val_acc: 0.2333\n",
      "Epoch 144/150\n",
      "119/119 [==============================] - 0s 149us/step - loss: 1.0956 - acc: 0.2941 - val_loss: 1.0985 - val_acc: 0.2333\n",
      "Epoch 145/150\n",
      "119/119 [==============================] - 0s 117us/step - loss: 1.0956 - acc: 0.2941 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 146/150\n",
      "119/119 [==============================] - 0s 141us/step - loss: 1.0953 - acc: 0.3361 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 147/150\n",
      "119/119 [==============================] - 0s 152us/step - loss: 1.0953 - acc: 0.3361 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 148/150\n",
      "119/119 [==============================] - 0s 139us/step - loss: 1.0953 - acc: 0.3361 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 149/150\n",
      "119/119 [==============================] - 0s 145us/step - loss: 1.0953 - acc: 0.3361 - val_loss: 1.0977 - val_acc: 0.3333\n",
      "Epoch 150/150\n",
      "119/119 [==============================] - 0s 138us/step - loss: 1.0953 - acc: 0.3361 - val_loss: 1.0977 - val_acc: 0.3333\n"
     ]
    }
   ],
   "source": [
    "#fitting the model and predicting \n",
    "history = model.fit(X_train,y_train,epochs=150, validation_data=(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.33      1.00      0.50        10\n",
      "           2       0.00      0.00      0.00         8\n",
      "\n",
      "   micro avg       0.33      0.33      0.33        30\n",
      "   macro avg       0.11      0.33      0.17        30\n",
      "weighted avg       0.11      0.33      0.17        30\n",
      "\n",
      "[[ 0 12  0]\n",
      " [ 0 10  0]\n",
      " [ 0  8  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haydenwang/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of the predicted values\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test_class,y_pred_class))\n",
    "print(confusion_matrix(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
