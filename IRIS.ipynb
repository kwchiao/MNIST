{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb\n",
      "Untitled1.ipynb\n",
      "hw1\n",
      "hw2\n",
      "iris.data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "print(check_output([\"ls\", \"./\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5.1</th>\n",
       "      <th>3.5</th>\n",
       "      <th>1.4</th>\n",
       "      <th>0.2</th>\n",
       "      <th>Iris-setosa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     5.1  3.5  1.4  0.2     Iris-setosa\n",
       "0    4.9  3.0  1.4  0.2     Iris-setosa\n",
       "1    4.7  3.2  1.3  0.2     Iris-setosa\n",
       "2    4.6  3.1  1.5  0.2     Iris-setosa\n",
       "3    5.0  3.6  1.4  0.2     Iris-setosa\n",
       "4    5.4  3.9  1.7  0.4     Iris-setosa\n",
       "5    4.6  3.4  1.4  0.3     Iris-setosa\n",
       "6    5.0  3.4  1.5  0.2     Iris-setosa\n",
       "7    4.4  2.9  1.4  0.2     Iris-setosa\n",
       "8    4.9  3.1  1.5  0.1     Iris-setosa\n",
       "9    5.4  3.7  1.5  0.2     Iris-setosa\n",
       "10   4.8  3.4  1.6  0.2     Iris-setosa\n",
       "11   4.8  3.0  1.4  0.1     Iris-setosa\n",
       "12   4.3  3.0  1.1  0.1     Iris-setosa\n",
       "13   5.8  4.0  1.2  0.2     Iris-setosa\n",
       "14   5.7  4.4  1.5  0.4     Iris-setosa\n",
       "15   5.4  3.9  1.3  0.4     Iris-setosa\n",
       "16   5.1  3.5  1.4  0.3     Iris-setosa\n",
       "17   5.7  3.8  1.7  0.3     Iris-setosa\n",
       "18   5.1  3.8  1.5  0.3     Iris-setosa\n",
       "19   5.4  3.4  1.7  0.2     Iris-setosa\n",
       "20   5.1  3.7  1.5  0.4     Iris-setosa\n",
       "21   4.6  3.6  1.0  0.2     Iris-setosa\n",
       "22   5.1  3.3  1.7  0.5     Iris-setosa\n",
       "23   4.8  3.4  1.9  0.2     Iris-setosa\n",
       "24   5.0  3.0  1.6  0.2     Iris-setosa\n",
       "25   5.0  3.4  1.6  0.4     Iris-setosa\n",
       "26   5.2  3.5  1.5  0.2     Iris-setosa\n",
       "27   5.2  3.4  1.4  0.2     Iris-setosa\n",
       "28   4.7  3.2  1.6  0.2     Iris-setosa\n",
       "29   4.8  3.1  1.6  0.2     Iris-setosa\n",
       "..   ...  ...  ...  ...             ...\n",
       "119  6.9  3.2  5.7  2.3  Iris-virginica\n",
       "120  5.6  2.8  4.9  2.0  Iris-virginica\n",
       "121  7.7  2.8  6.7  2.0  Iris-virginica\n",
       "122  6.3  2.7  4.9  1.8  Iris-virginica\n",
       "123  6.7  3.3  5.7  2.1  Iris-virginica\n",
       "124  7.2  3.2  6.0  1.8  Iris-virginica\n",
       "125  6.2  2.8  4.8  1.8  Iris-virginica\n",
       "126  6.1  3.0  4.9  1.8  Iris-virginica\n",
       "127  6.4  2.8  5.6  2.1  Iris-virginica\n",
       "128  7.2  3.0  5.8  1.6  Iris-virginica\n",
       "129  7.4  2.8  6.1  1.9  Iris-virginica\n",
       "130  7.9  3.8  6.4  2.0  Iris-virginica\n",
       "131  6.4  2.8  5.6  2.2  Iris-virginica\n",
       "132  6.3  2.8  5.1  1.5  Iris-virginica\n",
       "133  6.1  2.6  5.6  1.4  Iris-virginica\n",
       "134  7.7  3.0  6.1  2.3  Iris-virginica\n",
       "135  6.3  3.4  5.6  2.4  Iris-virginica\n",
       "136  6.4  3.1  5.5  1.8  Iris-virginica\n",
       "137  6.0  3.0  4.8  1.8  Iris-virginica\n",
       "138  6.9  3.1  5.4  2.1  Iris-virginica\n",
       "139  6.7  3.1  5.6  2.4  Iris-virginica\n",
       "140  6.9  3.1  5.1  2.3  Iris-virginica\n",
       "141  5.8  2.7  5.1  1.9  Iris-virginica\n",
       "142  6.8  3.2  5.9  2.3  Iris-virginica\n",
       "143  6.7  3.3  5.7  2.5  Iris-virginica\n",
       "144  6.7  3.0  5.2  2.3  Iris-virginica\n",
       "145  6.3  2.5  5.0  1.9  Iris-virginica\n",
       "146  6.5  3.0  5.2  2.0  Iris-virginica\n",
       "147  6.2  3.4  5.4  2.3  Iris-virginica\n",
       "148  5.9  3.0  5.1  1.8  Iris-virginica\n",
       "\n",
       "[149 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "dataset = pd.read_csv('./iris.data')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into training and test test\n",
    "X = dataset.iloc[:,1:4].values\n",
    "y = dataset.iloc[:,4].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder =  LabelEncoder()\n",
    "y1 = encoder.fit_transform(y)\n",
    "Y = pd.get_dummies(y1).values\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test, y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=0) \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "x_scaler = MinMaxScaler()\n",
    "X_train = x_scaler.fit_transform(X_train)\n",
    "X_test = x_scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Defining the model \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD,Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(10,input_shape=(4,),activation='tanh'))\n",
    "# model.add(Dense(8,activation='tanh'))\n",
    "# model.add(Dense(6,activation='tanh'))\n",
    "model.add(Dense(3,input_shape=(3,),activation='softmax'))\n",
    "\n",
    "model.compile(Adam(lr=0.04),'categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119 samples, validate on 30 samples\n",
      "Epoch 1/100\n",
      "119/119 [==============================] - 1s 5ms/step - loss: 0.8661 - acc: 0.5630 - val_loss: 0.7970 - val_acc: 0.6667\n",
      "Epoch 2/100\n",
      "119/119 [==============================] - 0s 108us/step - loss: 0.8006 - acc: 0.6639 - val_loss: 0.7558 - val_acc: 0.6667\n",
      "Epoch 3/100\n",
      "119/119 [==============================] - 0s 133us/step - loss: 0.7503 - acc: 0.6639 - val_loss: 0.7098 - val_acc: 0.6667\n",
      "Epoch 4/100\n",
      "119/119 [==============================] - 0s 151us/step - loss: 0.7064 - acc: 0.6639 - val_loss: 0.6632 - val_acc: 0.6667\n",
      "Epoch 5/100\n",
      "119/119 [==============================] - 0s 153us/step - loss: 0.6672 - acc: 0.6639 - val_loss: 0.6281 - val_acc: 0.6667\n",
      "Epoch 6/100\n",
      "119/119 [==============================] - 0s 192us/step - loss: 0.6348 - acc: 0.6891 - val_loss: 0.6001 - val_acc: 0.7000\n",
      "Epoch 7/100\n",
      "119/119 [==============================] - 0s 199us/step - loss: 0.6066 - acc: 0.6975 - val_loss: 0.5793 - val_acc: 0.7000\n",
      "Epoch 8/100\n",
      "119/119 [==============================] - 0s 158us/step - loss: 0.5811 - acc: 0.7059 - val_loss: 0.5577 - val_acc: 0.7333\n",
      "Epoch 9/100\n",
      "119/119 [==============================] - 0s 171us/step - loss: 0.5591 - acc: 0.7395 - val_loss: 0.5404 - val_acc: 0.7333\n",
      "Epoch 10/100\n",
      "119/119 [==============================] - 0s 158us/step - loss: 0.5402 - acc: 0.7647 - val_loss: 0.5279 - val_acc: 0.7333\n",
      "Epoch 11/100\n",
      "119/119 [==============================] - 0s 156us/step - loss: 0.5216 - acc: 0.7815 - val_loss: 0.5132 - val_acc: 0.7333\n",
      "Epoch 12/100\n",
      "119/119 [==============================] - 0s 151us/step - loss: 0.5056 - acc: 0.8319 - val_loss: 0.4982 - val_acc: 0.7333\n",
      "Epoch 13/100\n",
      "119/119 [==============================] - 0s 164us/step - loss: 0.4918 - acc: 0.8487 - val_loss: 0.4847 - val_acc: 0.7333\n",
      "Epoch 14/100\n",
      "119/119 [==============================] - 0s 161us/step - loss: 0.4788 - acc: 0.8571 - val_loss: 0.4790 - val_acc: 0.7333\n",
      "Epoch 15/100\n",
      "119/119 [==============================] - 0s 195us/step - loss: 0.4657 - acc: 0.8571 - val_loss: 0.4701 - val_acc: 0.7333\n",
      "Epoch 16/100\n",
      "119/119 [==============================] - 0s 149us/step - loss: 0.4562 - acc: 0.8571 - val_loss: 0.4664 - val_acc: 0.7333\n",
      "Epoch 17/100\n",
      "119/119 [==============================] - 0s 173us/step - loss: 0.4453 - acc: 0.8655 - val_loss: 0.4546 - val_acc: 0.7333\n",
      "Epoch 18/100\n",
      "119/119 [==============================] - 0s 171us/step - loss: 0.4346 - acc: 0.8739 - val_loss: 0.4499 - val_acc: 0.7333\n",
      "Epoch 19/100\n",
      "119/119 [==============================] - 0s 189us/step - loss: 0.4264 - acc: 0.8739 - val_loss: 0.4416 - val_acc: 0.7333\n",
      "Epoch 20/100\n",
      "119/119 [==============================] - 0s 159us/step - loss: 0.4181 - acc: 0.8824 - val_loss: 0.4399 - val_acc: 0.7333\n",
      "Epoch 21/100\n",
      "119/119 [==============================] - 0s 183us/step - loss: 0.4094 - acc: 0.8992 - val_loss: 0.4323 - val_acc: 0.7333\n",
      "Epoch 22/100\n",
      "119/119 [==============================] - 0s 161us/step - loss: 0.4018 - acc: 0.9160 - val_loss: 0.4264 - val_acc: 0.7000\n",
      "Epoch 23/100\n",
      "119/119 [==============================] - 0s 136us/step - loss: 0.3951 - acc: 0.9160 - val_loss: 0.4192 - val_acc: 0.7667\n",
      "Epoch 24/100\n",
      "119/119 [==============================] - 0s 166us/step - loss: 0.3899 - acc: 0.9160 - val_loss: 0.4188 - val_acc: 0.7333\n",
      "Epoch 25/100\n",
      "119/119 [==============================] - 0s 107us/step - loss: 0.3826 - acc: 0.9244 - val_loss: 0.4116 - val_acc: 0.8000\n",
      "Epoch 26/100\n",
      "119/119 [==============================] - 0s 164us/step - loss: 0.3762 - acc: 0.9160 - val_loss: 0.4121 - val_acc: 0.7333\n",
      "Epoch 27/100\n",
      "119/119 [==============================] - 0s 151us/step - loss: 0.3709 - acc: 0.9160 - val_loss: 0.4110 - val_acc: 0.7000\n",
      "Epoch 28/100\n",
      "119/119 [==============================] - 0s 147us/step - loss: 0.3643 - acc: 0.9160 - val_loss: 0.4043 - val_acc: 0.7667\n",
      "Epoch 29/100\n",
      "119/119 [==============================] - 0s 157us/step - loss: 0.3590 - acc: 0.9328 - val_loss: 0.3974 - val_acc: 0.8333\n",
      "Epoch 30/100\n",
      "119/119 [==============================] - 0s 147us/step - loss: 0.3545 - acc: 0.9496 - val_loss: 0.3913 - val_acc: 0.8333\n",
      "Epoch 31/100\n",
      "119/119 [==============================] - 0s 157us/step - loss: 0.3498 - acc: 0.9412 - val_loss: 0.3916 - val_acc: 0.8333\n",
      "Epoch 32/100\n",
      "119/119 [==============================] - 0s 176us/step - loss: 0.3447 - acc: 0.9412 - val_loss: 0.3925 - val_acc: 0.8333\n",
      "Epoch 33/100\n",
      "119/119 [==============================] - 0s 144us/step - loss: 0.3403 - acc: 0.9328 - val_loss: 0.3923 - val_acc: 0.8000\n",
      "Epoch 34/100\n",
      "119/119 [==============================] - 0s 143us/step - loss: 0.3355 - acc: 0.9328 - val_loss: 0.3866 - val_acc: 0.8333\n",
      "Epoch 35/100\n",
      "119/119 [==============================] - 0s 164us/step - loss: 0.3313 - acc: 0.9412 - val_loss: 0.3810 - val_acc: 0.8333\n",
      "Epoch 36/100\n",
      "119/119 [==============================] - 0s 140us/step - loss: 0.3286 - acc: 0.9328 - val_loss: 0.3827 - val_acc: 0.8333\n",
      "Epoch 37/100\n",
      "119/119 [==============================] - 0s 171us/step - loss: 0.3235 - acc: 0.9412 - val_loss: 0.3780 - val_acc: 0.8333\n",
      "Epoch 38/100\n",
      "119/119 [==============================] - 0s 161us/step - loss: 0.3194 - acc: 0.9412 - val_loss: 0.3757 - val_acc: 0.8333\n",
      "Epoch 39/100\n",
      "119/119 [==============================] - 0s 165us/step - loss: 0.3154 - acc: 0.9412 - val_loss: 0.3725 - val_acc: 0.8333\n",
      "Epoch 40/100\n",
      "119/119 [==============================] - 0s 144us/step - loss: 0.3118 - acc: 0.9412 - val_loss: 0.3680 - val_acc: 0.8333\n",
      "Epoch 41/100\n",
      "119/119 [==============================] - 0s 140us/step - loss: 0.3082 - acc: 0.9580 - val_loss: 0.3639 - val_acc: 0.8333\n",
      "Epoch 42/100\n",
      "119/119 [==============================] - 0s 151us/step - loss: 0.3048 - acc: 0.9580 - val_loss: 0.3624 - val_acc: 0.8333\n",
      "Epoch 43/100\n",
      "119/119 [==============================] - 0s 153us/step - loss: 0.3013 - acc: 0.9580 - val_loss: 0.3633 - val_acc: 0.8333\n",
      "Epoch 44/100\n",
      "119/119 [==============================] - 0s 155us/step - loss: 0.2977 - acc: 0.9412 - val_loss: 0.3651 - val_acc: 0.8333\n",
      "Epoch 45/100\n",
      "119/119 [==============================] - 0s 168us/step - loss: 0.2951 - acc: 0.9412 - val_loss: 0.3643 - val_acc: 0.8333\n",
      "Epoch 46/100\n",
      "119/119 [==============================] - 0s 168us/step - loss: 0.2922 - acc: 0.9412 - val_loss: 0.3597 - val_acc: 0.8333\n",
      "Epoch 47/100\n",
      "119/119 [==============================] - 0s 172us/step - loss: 0.2887 - acc: 0.9496 - val_loss: 0.3527 - val_acc: 0.8333\n",
      "Epoch 48/100\n",
      "119/119 [==============================] - 0s 136us/step - loss: 0.2865 - acc: 0.9664 - val_loss: 0.3452 - val_acc: 0.8333\n",
      "Epoch 49/100\n",
      "119/119 [==============================] - 0s 133us/step - loss: 0.2851 - acc: 0.9496 - val_loss: 0.3485 - val_acc: 0.8333\n",
      "Epoch 50/100\n",
      "119/119 [==============================] - 0s 154us/step - loss: 0.2806 - acc: 0.9580 - val_loss: 0.3413 - val_acc: 0.8333\n",
      "Epoch 51/100\n",
      "119/119 [==============================] - 0s 134us/step - loss: 0.2773 - acc: 0.9664 - val_loss: 0.3377 - val_acc: 0.8333\n",
      "Epoch 52/100\n",
      "119/119 [==============================] - 0s 153us/step - loss: 0.2751 - acc: 0.9664 - val_loss: 0.3377 - val_acc: 0.8333\n",
      "Epoch 53/100\n",
      "119/119 [==============================] - 0s 131us/step - loss: 0.2718 - acc: 0.9664 - val_loss: 0.3394 - val_acc: 0.8333\n",
      "Epoch 54/100\n",
      "119/119 [==============================] - 0s 138us/step - loss: 0.2693 - acc: 0.9664 - val_loss: 0.3374 - val_acc: 0.8333\n",
      "Epoch 55/100\n",
      "119/119 [==============================] - 0s 113us/step - loss: 0.2677 - acc: 0.9580 - val_loss: 0.3396 - val_acc: 0.8333\n",
      "Epoch 56/100\n",
      "119/119 [==============================] - 0s 132us/step - loss: 0.2644 - acc: 0.9580 - val_loss: 0.3331 - val_acc: 0.8333\n",
      "Epoch 57/100\n",
      "119/119 [==============================] - 0s 128us/step - loss: 0.2622 - acc: 0.9664 - val_loss: 0.3254 - val_acc: 0.8667\n",
      "Epoch 58/100\n",
      "119/119 [==============================] - 0s 132us/step - loss: 0.2600 - acc: 0.9664 - val_loss: 0.3259 - val_acc: 0.8333\n",
      "Epoch 59/100\n",
      "119/119 [==============================] - 0s 113us/step - loss: 0.2571 - acc: 0.9664 - val_loss: 0.3217 - val_acc: 0.8667\n",
      "Epoch 60/100\n",
      "119/119 [==============================] - 0s 128us/step - loss: 0.2559 - acc: 0.9664 - val_loss: 0.3184 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "119/119 [==============================] - 0s 133us/step - loss: 0.2543 - acc: 0.9664 - val_loss: 0.3271 - val_acc: 0.8333\n",
      "Epoch 62/100\n",
      "119/119 [==============================] - 0s 115us/step - loss: 0.2511 - acc: 0.9496 - val_loss: 0.3263 - val_acc: 0.8333\n",
      "Epoch 63/100\n",
      "119/119 [==============================] - 0s 132us/step - loss: 0.2482 - acc: 0.9580 - val_loss: 0.3173 - val_acc: 0.8333\n",
      "Epoch 64/100\n",
      "119/119 [==============================] - 0s 178us/step - loss: 0.2456 - acc: 0.9664 - val_loss: 0.3096 - val_acc: 0.9000\n",
      "Epoch 65/100\n",
      "119/119 [==============================] - 0s 165us/step - loss: 0.2441 - acc: 0.9664 - val_loss: 0.3057 - val_acc: 0.9000\n",
      "Epoch 66/100\n",
      "119/119 [==============================] - 0s 170us/step - loss: 0.2425 - acc: 0.9664 - val_loss: 0.3034 - val_acc: 0.9000\n",
      "Epoch 67/100\n",
      "119/119 [==============================] - 0s 193us/step - loss: 0.2398 - acc: 0.9664 - val_loss: 0.3053 - val_acc: 0.9000\n",
      "Epoch 68/100\n",
      "119/119 [==============================] - 0s 168us/step - loss: 0.2387 - acc: 0.9664 - val_loss: 0.3112 - val_acc: 0.8333\n",
      "Epoch 69/100\n",
      "119/119 [==============================] - 0s 150us/step - loss: 0.2365 - acc: 0.9580 - val_loss: 0.3092 - val_acc: 0.8333\n",
      "Epoch 70/100\n",
      "119/119 [==============================] - 0s 152us/step - loss: 0.2346 - acc: 0.9664 - val_loss: 0.3004 - val_acc: 0.9000\n",
      "Epoch 71/100\n",
      "119/119 [==============================] - 0s 176us/step - loss: 0.2330 - acc: 0.9664 - val_loss: 0.2934 - val_acc: 0.9000\n",
      "Epoch 72/100\n",
      "119/119 [==============================] - 0s 170us/step - loss: 0.2305 - acc: 0.9748 - val_loss: 0.2922 - val_acc: 0.9000\n",
      "Epoch 73/100\n",
      "119/119 [==============================] - 0s 162us/step - loss: 0.2286 - acc: 0.9748 - val_loss: 0.2922 - val_acc: 0.9000\n",
      "Epoch 74/100\n",
      "119/119 [==============================] - 0s 152us/step - loss: 0.2265 - acc: 0.9664 - val_loss: 0.2925 - val_acc: 0.9000\n",
      "Epoch 75/100\n",
      "119/119 [==============================] - 0s 192us/step - loss: 0.2249 - acc: 0.9664 - val_loss: 0.2927 - val_acc: 0.9000\n",
      "Epoch 76/100\n",
      "119/119 [==============================] - 0s 134us/step - loss: 0.2232 - acc: 0.9580 - val_loss: 0.2925 - val_acc: 0.9000\n",
      "Epoch 77/100\n",
      "119/119 [==============================] - 0s 156us/step - loss: 0.2216 - acc: 0.9580 - val_loss: 0.2893 - val_acc: 0.9000\n",
      "Epoch 78/100\n",
      "119/119 [==============================] - 0s 153us/step - loss: 0.2199 - acc: 0.9664 - val_loss: 0.2848 - val_acc: 0.9000\n",
      "Epoch 79/100\n",
      "119/119 [==============================] - 0s 157us/step - loss: 0.2181 - acc: 0.9664 - val_loss: 0.2815 - val_acc: 0.9000\n",
      "Epoch 80/100\n",
      "119/119 [==============================] - 0s 133us/step - loss: 0.2165 - acc: 0.9748 - val_loss: 0.2786 - val_acc: 0.9000\n",
      "Epoch 81/100\n",
      "119/119 [==============================] - 0s 203us/step - loss: 0.2149 - acc: 0.9748 - val_loss: 0.2756 - val_acc: 0.9000\n",
      "Epoch 82/100\n",
      "119/119 [==============================] - 0s 135us/step - loss: 0.2133 - acc: 0.9748 - val_loss: 0.2727 - val_acc: 0.9000\n",
      "Epoch 83/100\n",
      "119/119 [==============================] - 0s 150us/step - loss: 0.2126 - acc: 0.9748 - val_loss: 0.2702 - val_acc: 0.9000\n",
      "Epoch 84/100\n",
      "119/119 [==============================] - 0s 142us/step - loss: 0.2103 - acc: 0.9748 - val_loss: 0.2730 - val_acc: 0.9000\n",
      "Epoch 85/100\n",
      "119/119 [==============================] - 0s 167us/step - loss: 0.2090 - acc: 0.9748 - val_loss: 0.2756 - val_acc: 0.9333\n",
      "Epoch 86/100\n",
      "119/119 [==============================] - 0s 157us/step - loss: 0.2080 - acc: 0.9580 - val_loss: 0.2752 - val_acc: 0.9333\n",
      "Epoch 87/100\n",
      "119/119 [==============================] - 0s 177us/step - loss: 0.2096 - acc: 0.9664 - val_loss: 0.2658 - val_acc: 0.9000\n",
      "Epoch 88/100\n",
      "119/119 [==============================] - 0s 143us/step - loss: 0.2043 - acc: 0.9748 - val_loss: 0.2642 - val_acc: 0.9000\n",
      "Epoch 89/100\n",
      "119/119 [==============================] - 0s 140us/step - loss: 0.2033 - acc: 0.9748 - val_loss: 0.2620 - val_acc: 0.9000\n",
      "Epoch 90/100\n",
      "119/119 [==============================] - 0s 119us/step - loss: 0.2018 - acc: 0.9748 - val_loss: 0.2645 - val_acc: 0.9000\n",
      "Epoch 91/100\n",
      "119/119 [==============================] - 0s 131us/step - loss: 0.2008 - acc: 0.9664 - val_loss: 0.2663 - val_acc: 0.9333\n",
      "Epoch 92/100\n",
      "119/119 [==============================] - 0s 151us/step - loss: 0.1997 - acc: 0.9664 - val_loss: 0.2607 - val_acc: 0.9000\n",
      "Epoch 93/100\n",
      "119/119 [==============================] - 0s 163us/step - loss: 0.1975 - acc: 0.9748 - val_loss: 0.2594 - val_acc: 0.9000\n",
      "Epoch 94/100\n",
      "119/119 [==============================] - 0s 139us/step - loss: 0.1964 - acc: 0.9748 - val_loss: 0.2568 - val_acc: 0.9000\n",
      "Epoch 95/100\n",
      "119/119 [==============================] - 0s 146us/step - loss: 0.1963 - acc: 0.9748 - val_loss: 0.2520 - val_acc: 0.9333\n",
      "Epoch 96/100\n",
      "119/119 [==============================] - 0s 153us/step - loss: 0.1944 - acc: 0.9748 - val_loss: 0.2539 - val_acc: 0.9000\n",
      "Epoch 97/100\n",
      "119/119 [==============================] - 0s 154us/step - loss: 0.1929 - acc: 0.9748 - val_loss: 0.2540 - val_acc: 0.9333\n",
      "Epoch 98/100\n",
      "119/119 [==============================] - 0s 156us/step - loss: 0.1923 - acc: 0.9664 - val_loss: 0.2548 - val_acc: 0.9333\n",
      "Epoch 99/100\n",
      "119/119 [==============================] - 0s 163us/step - loss: 0.1902 - acc: 0.9664 - val_loss: 0.2499 - val_acc: 0.9333\n",
      "Epoch 100/100\n",
      "119/119 [==============================] - 0s 149us/step - loss: 0.1890 - acc: 0.9748 - val_loss: 0.2462 - val_acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "#fitting the model and predicting \n",
    "history = model.fit(X_train,y_train,epochs=100, validation_data=(X_test, y_test))\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_test_class = np.argmax(y_test,axis=1)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.90      0.90      0.90        10\n",
      "           2       0.88      0.88      0.88         8\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        30\n",
      "   macro avg       0.92      0.92      0.92        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "[[12  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  1  7]]\n"
     ]
    }
   ],
   "source": [
    "#Accuracy of the predicted values\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(y_test_class,y_pred_class))\n",
    "print(confusion_matrix(y_test_class,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
